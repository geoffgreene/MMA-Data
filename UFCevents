# This Python code scrapes the details of every UFC event from the "List of UFC events" Wikipedia page, 
# populates a pandas dataframe with the data, and saves the dataframe to a csv file. 

import requests
import lxml.html as lh
import pandas as pd

url='https://en.wikipedia.org/wiki/List_of_UFC_events#Past_events'

#Create a handle, page, to handle the contents of the website
page = requests.get(url)

#Store the contents of the website under doc
doc = lh.fromstring(page.content)

#Parse data that are stored between <tr>..</tr> of HTML
tr_elements = doc.xpath('//tr')

# The table of past events has 7 columns. We create a list and add only the row elements that are of length 7.
events = []
for t in tr_elements:
    if len(t) == 7:
       events.append(t)
       
#Create empty list to store data
col=[]
i=0
#For each row, store each first element (header) and an empty list
for t in events[0]:
    i+=1
    name=t.text_content()
    col.append((name,[]))
    
#Since our first row is the header, data is stored from the second row onwards
for j in range(1,len(events)):
    #T is our j'th row
    T=events[j]
    
    #i is the index of our column
    i=0
    
    #Iterate through each element of the row
    for t in T.iterchildren():
        data=t.text_content() 
        #Check if row is empty
        if i>0:
        #Convert any numerical value to integers
            try:
                data=int(data)
            except:
                pass
        #Append the data to the empty list of the i'th column
        col[i][1].append(data)
        #Increment i for the next column
        i+=1    
    
# Create a dictionary from the extracted data
Dict={title:column for (title,column) in col}

#Create a pandas dataframe from the dictionary
df=pd.DataFrame(Dict)

# Clean the data by removing the "\n" suffix attached to each cell element
df = df.replace('\n',' ', regex=True)

# Save the dataframe to a CSV file
df.to_csv(r'ufcevents.csv')
